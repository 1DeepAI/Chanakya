{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1660de2d-148f-4171-a2d2-13c0270007b1",
   "metadata": {},
   "source": [
    "# Python Practice 481-490"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55139cfd-7642-41e6-a555-ed261db5e3a4",
   "metadata": {},
   "source": [
    "## Here are Python Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ea9824-f198-4d72-8867-5ddd9dcdc09e",
   "metadata": {},
   "source": [
    "### 481. Develop a Reinforcement Learning Agent using Truncated Importance Sampling with Custom Importance Sampling Ratio\n",
    " Importance sampling is a method in reinforcement learning to correct the bias when learning from a behavior policy that's different from the target policy. Truncated importance sampling limits the weight of the importance sampling ratio to avoid high variance.\n",
    "\n",
    "Below is a simple Python program to demonstrate the concept with a toy problem. The RL agent will try to maximize its reward in a random walk environment:\n",
    "\n",
    "Environment: The agent starts in the middle of a 5-state line (states: A, B, C, D, E). The agent can move left or right. Moving out of state A or state E ends the episode.\n",
    "Reward: The agent receives a reward of +1 if it moves out of state E and a reward of -1 if it moves out of state A.\n",
    "We'll use Q-learning with truncated importance sampling. The agent will have a behavior policy (epsilon-greedy) different from the target policy (greedy).\n",
    "\n",
    "Expected Output:\n",
    "You should see a 2D array (Q-values) which should have higher values on the right-side actions (moving towards state E) as it leads to a reward of +1.\n",
    "\n",
    "Note:\n",
    "\n",
    "This is a toy example to demonstrate the truncated importance sampling concept. In more complex environments, additional considerations (like neural networks for approximation, replay buffers, etc.) would be necessary.\n",
    "EPSILON is set to 0.5 here for demonstration. In practice, it might be useful to decay the EPSILON over time to reduce exploration and exploit more as the agent learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0a45bf-df29-4c57-87f9-21b0349b86e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "ALPHA = 0.1  # Learning rate\n",
    "EPSILON = 0.5  # Exploration rate for behavior policy\n",
    "GAMMA = 1.0  # Discount factor\n",
    "EPISODES = 5000\n",
    "TRUNCATION_LIMIT = 2.0\n",
    "\n",
    "# States A, B, C, D, E\n",
    "state_space = ['A', 'B', 'C', 'D', 'E']\n",
    "n_states = len(state_space)\n",
    "\n",
    "# Actions: 0 (left), 1 (right)\n",
    "n_actions = 2\n",
    "\n",
    "# Q-values\n",
    "Q = np.zeros((n_states, n_actions))\n",
    "\n",
    "# Transitions and rewards\n",
    "transitions = {'A': {'0': (-1, -1), '1': (1, 0)},\n",
    "               'B': {'0': (0, 0), '1': (2, 0)},\n",
    "               'C': {'0': (1, 0), '1': (3, 0)},\n",
    "               'D': {'0': (2, 0), '1': (4, 0)},\n",
    "               'E': {'0': (3, 0), '1': (5, 1)}}\n",
    "\n",
    "def behavior_policy(state):\n",
    "    if np.random.rand() < EPSILON:\n",
    "        return np.random.choice([0, 1])\n",
    "    return np.argmax(Q[state, :])\n",
    "\n",
    "def target_policy(state):\n",
    "    return np.argmax(Q[state, :])\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    state = 2  # Start at C\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = behavior_policy(state)\n",
    "        next_state, reward = transitions[state_space[state]][str(action)]\n",
    "        \n",
    "        # Truncated Importance Sampling\n",
    "        if action == target_policy(state):\n",
    "            rho = 1.0 / (1.0 - EPSILON + EPSILON/n_actions)\n",
    "        else:\n",
    "            rho = 1.0 / (EPSILON/n_actions)\n",
    "        \n",
    "        # Truncate the importance sampling ratio\n",
    "        rho = min(TRUNCATION_LIMIT, rho)\n",
    "\n",
    "        # Q-learning update\n",
    "        best_next_action = np.argmax(Q[next_state, :])\n",
    "        td_target = reward + GAMMA * Q[next_state, best_next_action]\n",
    "        td_delta = td_target - Q[state, action]\n",
    "        Q[state, action] += ALPHA * rho * td_delta\n",
    "\n",
    "        state = next_state\n",
    "        if state == -1 or state == 5:\n",
    "            done = True\n",
    "\n",
    "print(Q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c272078d-63e9-430a-a0dc-2740726095d0",
   "metadata": {},
   "source": [
    "### 482. Build a Recommender System with Cross-Domain Recommendation and Custom Domain Alignment\n",
    "Cross-domain recommendation is the task of transferring the knowledge learned in a source domain to improve the recommendation performance in a target domain. One way to perform cross-domain recommendation is to utilize shared latent factors or embeddings that are common across the domains. These shared embeddings can align the two domains, which can help in making better recommendations.\n",
    "\n",
    "For simplicity, let's build a recommender system using matrix factorization and align the embeddings for two domains (books and movies). We'll simulate the data for the two domains:\n",
    "\n",
    "1. Utilize matrix factorization to get user and item embeddings for both domains.\n",
    "2. Align the embeddings by minimizing the distance between source and target domain embeddings.\n",
    "3. Use the aligned embeddings for recommendation in the target domain.\n",
    "\n",
    "\n",
    "Expected Output:\n",
    "The output will show embeddings for a new user in the book and movie domain. They should be somewhat similar because of the alignment layer, but won't be identical due to the randomness in our synthetic data and embeddings.\n",
    "\n",
    "Note:\n",
    "\n",
    "This is a simplistic demonstration, and in practice, more sophisticated methods and a lot more data are used.\n",
    "The embeddings are aligned by forcing them through the same transformation layer. This simplistic approach ensures that the embeddings for users are somewhat aligned across domains.\n",
    "Cross-domain recommendation is a complex topic, and this example serves just as an introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0498e36-7864-41b4-b640-ac731a173b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Simulated data\n",
    "users = 100\n",
    "books = 50\n",
    "movies = 60\n",
    "latent_factors = 10\n",
    "\n",
    "# Random user-book interactions (source domain)\n",
    "book_ratings = np.random.randint(0, 6, (users, books))\n",
    "\n",
    "# Random user-movie interactions (target domain)\n",
    "movie_ratings = np.random.randint(0, 6, (users, movies))\n",
    "\n",
    "# Matrix Factorization for both domains\n",
    "book_input = keras.layers.Input(shape=(books,))\n",
    "book_emb = keras.layers.Dense(latent_factors, activation='relu')(book_input)\n",
    "\n",
    "movie_input = keras.layers.Input(shape=(movies,))\n",
    "movie_emb = keras.layers.Dense(latent_factors, activation='relu')(movie_input)\n",
    "\n",
    "# Domain alignment layer - here, we'll just use a dense layer\n",
    "alignment_layer = keras.layers.Dense(latent_factors)\n",
    "\n",
    "aligned_book_emb = alignment_layer(book_emb)\n",
    "aligned_movie_emb = alignment_layer(movie_emb)\n",
    "\n",
    "# Model\n",
    "book_model = keras.models.Model(inputs=book_input, outputs=aligned_book_emb)\n",
    "movie_model = keras.models.Model(inputs=movie_input, outputs=aligned_movie_emb)\n",
    "\n",
    "book_model.compile(optimizer='adam', loss='mse')\n",
    "movie_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Simulated training (just to create embeddings)\n",
    "book_model.fit(book_ratings, np.random.random((users, latent_factors)), epochs=5, verbose=0)\n",
    "movie_model.fit(movie_ratings, np.random.random((users, latent_factors)), epochs=5, verbose=0)\n",
    "\n",
    "# After training, you can utilize these models to make recommendations in each domain.\n",
    "# With the embeddings aligned, knowledge from the book domain can assist in making better recommendations in the movie domain.\n",
    "\n",
    "# Example: Get embeddings for a new user\n",
    "new_user_books = np.random.randint(0, 6, (1, books))\n",
    "new_user_movies = np.random.randint(0, 6, (1, movies))\n",
    "\n",
    "book_embedding = book_model.predict(new_user_books)\n",
    "movie_embedding = movie_model.predict(new_user_movies)\n",
    "\n",
    "print(\"Book Embedding for New User:\", book_embedding)\n",
    "print(\"Movie Embedding for New User:\", movie_embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d1d02b-5d75-414a-8e31-1a513e46541e",
   "metadata": {},
   "source": [
    "### 483. Implement a Transfer Learning Model with Zero-Shot Learning and Custom Semantic Embeddings\n",
    "Zero-Shot Learning (ZSL) is the task of making predictions for classes that have not been observed during training. One popular method for ZSL is to utilize semantic embeddings (like word vectors) to bridge the source and target classes.\n",
    "\n",
    "Below is an illustrative approach on how one might utilize Zero-Shot Learning with word embeddings using the Word2Vec model provided by gensim. We'll create a simple synthetic dataset for demonstration:\n",
    "\n",
    "1. Train a model on a source task.\n",
    "2. Create semantic embeddings for source and target classes.\n",
    "3. During inference, use the semantic similarity between source and target classes to make predictions.\n",
    "\n",
    "For the sake of simplicity, we'll assume a binary classification problem where the source task distinguishes between cat and dog, and the target task is to predict wolf.\n",
    "\n",
    "Expected Output:\n",
    "You'll get an array of class predictions for the \"wolf\" test samples based on the semantic similarity between \"cat\", \"dog\", and \"wolf\". Depending on the semantic similarities and the model's predictions, it will either lean towards the cat class or the dog class for the predictions.\n",
    "\n",
    "Note:\n",
    "\n",
    "1. Ensure you have TensorFlow, Gensim installed.\n",
    "2. You'll need to download the GoogleNews-vectors-negative300.bin Word2Vec model, which is a sizeable download (~3.5 GB). You can adjust the code to use other embeddings if desired.\n",
    "3. This is a simple demonstration. In practice, more sophisticated methods and more data are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc34600-0cc6-468a-a462-2221428107c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load word vectors (ensure you have downloaded GoogleNews-vectors-negative300.bin)\n",
    "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "# Synthetic dataset\n",
    "# 1: cat, 2: dog\n",
    "X_train = np.array([[1, 0], [0, 1], [1.1, 0.2], [0.2, 1.1]])\n",
    "y_train = np.array([0, 1, 0, 1])\n",
    "\n",
    "# Simple Feedforward Neural Network\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(5, activation='relu', input_shape=(2,)),\n",
    "    keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10)\n",
    "\n",
    "# Assume we've not seen \"wolf\" during training\n",
    "def zero_shot_predict(model, word_vectors, input_data, target_word):\n",
    "    predictions = model.predict(input_data)\n",
    "    # Compute semantic similarities\n",
    "    cat_similarity = word_vectors.similarity('cat', target_word)\n",
    "    dog_similarity = word_vectors.similarity('dog', target_word)\n",
    "    similarities = np.array([cat_similarity, dog_similarity])\n",
    "    # Scale predictions by similarities\n",
    "    adjusted_predictions = predictions * similarities\n",
    "    # Normalize to turn back into probability distribution\n",
    "    adjusted_predictions /= adjusted_predictions.sum(axis=1, keepdims=True)\n",
    "    return np.argmax(adjusted_predictions, axis=1)\n",
    "\n",
    "# Synthetic test data for \"wolf\"\n",
    "X_test = np.array([[0.5, 0.5], [0.6, 0.4]])\n",
    "predictions = zero_shot_predict(model, word_vectors, X_test, 'wolf')\n",
    "print(predictions) # Expected to print either class (cat or dog) based on semantic similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e68a9c-2a53-457c-bc65-38e0fd3a9f9f",
   "metadata": {},
   "source": [
    "### 484. Create a Reinforcement Learning Agent using Distributional Reinforcement Learning with Custom Distributional Q-Values\n",
    "Distributional Reinforcement Learning involves representing the Q-values as distributions rather than scalars. A popular algorithm that employs this technique is the C51 algorithm. In this example, I'll show a basic cartpole agent using a custom Distributional Q-value setup with TensorFlow.\n",
    "\n",
    "Expected Output:\n",
    "You should see the cumulative reward of the agent for each episode. With enough episodes and proper hyperparameter settings, the agent should achieve a near-optimal policy for the CartPole environment.\n",
    "\n",
    "Notes:\n",
    "\n",
    "1. Ensure TensorFlow and Gym are installed.\n",
    "2. The code employs a basic version of distributional RL, and there are more advanced techniques and algorithms in the literature.\n",
    "3. This example uses the CartPole environment for simplicity. Adjustments might be necessary for other environments.\n",
    "4. The model might need more epochs and tweaks for stable and high performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6b8eea-deea-4da8-b837-73e08842b65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import gym\n",
    "\n",
    "# Define environment\n",
    "env = gym.make('CartPole-v1')\n",
    "num_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "# Define hyperparameters\n",
    "gamma = 0.99\n",
    "learning_rate = 0.001\n",
    "num_atoms = 51\n",
    "v_min = -10.0\n",
    "v_max = 10.0\n",
    "delta_z = (v_max - v_min) / (num_atoms - 1)\n",
    "z = np.linspace(v_min, v_max, num_atoms)\n",
    "\n",
    "# Distributional Q-network\n",
    "def create_model():\n",
    "    inputs = layers.Input(shape=(state_dim,))\n",
    "    x = layers.Dense(24, activation='relu')(inputs)\n",
    "    x = layers.Dense(24, activation='relu')(x)\n",
    "    x = layers.Dense(num_actions * num_atoms, activation='softmax')(x)\n",
    "    return models.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "model = create_model()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Custom Q-value function using distributional values\n",
    "def get_distributional_q_values(state):\n",
    "    logits = model(state)\n",
    "    probs = tf.keras.activations.softmax(logits)\n",
    "    q_values = tf.reduce_sum(probs * z, axis=2)\n",
    "    return q_values\n",
    "\n",
    "# Update model\n",
    "def update(state, action, reward, next_state, done):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(state)\n",
    "        probs = tf.keras.activations.softmax(logits)\n",
    "        \n",
    "        next_logits = model(next_state)\n",
    "        next_probs = tf.keras.activations.softmax(next_logits)\n",
    "        next_q_values = tf.reduce_sum(next_probs * z, axis=2)\n",
    "        next_actions = tf.argmax(next_q_values, axis=1)\n",
    "        \n",
    "        target_probs = np.zeros_like(probs.numpy())\n",
    "        for i in range(probs.shape[0]):\n",
    "            if done[i]:\n",
    "                tz = min(max(reward[i], v_min), v_max)\n",
    "                bj = (tz - v_min) / delta_z\n",
    "                l, u = np.floor(bj), np.ceil(bj)\n",
    "                target_probs[i][action[i]][int(l)] += (u - bj)\n",
    "                target_probs[i][action[i]][int(u)] += (bj - l)\n",
    "            else:\n",
    "                for j in range(num_atoms):\n",
    "                    tz = min(max(reward[i] + gamma * z[j], v_min), v_max)\n",
    "                    bj = (tz - v_min) / delta_z\n",
    "                    l, u = np.floor(bj), np.ceil(bj)\n",
    "                    target_probs[i][action[i]][int(l)] += next_probs[i][next_actions[i]][j] * (u - bj)\n",
    "                    target_probs[i][action[i]][int(u)] += next_probs[i][next_actions[i]][j] * (bj - l)\n",
    "        \n",
    "        loss = -tf.reduce_sum(target_probs * tf.math.log(probs + 1e-10))\n",
    "        \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "# Train\n",
    "num_episodes = 200\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    while True:\n",
    "        action = np.argmax(get_distributional_q_values(state[None, :]))\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        update(state[None, :], [action], [reward], next_state[None, :], [done])\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    print(f\"Episode {episode + 1}: {episode_reward}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67873820-3b56-4ee6-8e5a-17b8c2579c21",
   "metadata": {},
   "source": [
    "### 485. Develop a Generative Adversarial Network (GAN) with Spectral Normalization for Image Generation\n",
    "Spectral normalization is a technique to stabilize the training of the discriminator in a GAN by constraining its Lipschitz constant. It's particularly useful to prevent the discriminator from becoming too powerful.\n",
    "\n",
    "\n",
    "Expected Output: \n",
    "The training will print the completed epochs.\n",
    "At the end of the training, 9 generated images from the generator will be displayed. These images should resemble handwritten digits if the training was successful.\n",
    "\n",
    "Notes:\n",
    "1. Ensure TensorFlow is installed.\n",
    "2. This is a basic example and may need further tuning and improvements to generate high-quality images.\n",
    "3. Spectral normalization is applied only to the Dense layer of the discriminator in this example. In a more comprehensive model, it would typically be applied to other layers as well.\n",
    "\n",
    "Here's a simple implementation of a GAN with spectral normalization using TensorFlow and Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907ddd7b-94a3-4cd5-b101-385126f90a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Sample data: MNIST\n",
    "(x_train, _), (_, _) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')\n",
    "x_train = (x_train - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
    "\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "# Create the Generator\n",
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 256)))\n",
    "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "    return model\n",
    "\n",
    "# Spectral Normalization\n",
    "def spectral_norm(w, iteration=1):\n",
    "    w_shape = w.shape.as_list()\n",
    "    w = tf.reshape(w, [-1, w_shape[-1]])\n",
    "\n",
    "    u = tf.random.normal([1, w_shape[-1]])\n",
    "    for _ in range(iteration):\n",
    "        v = tf.linalg.matvec(tf.transpose(w), u)\n",
    "        v = tf.nn.l2_normalize(v)\n",
    "        u = tf.linalg.matvec(w, v)\n",
    "        u = tf.nn.l2_normalize(u)\n",
    "\n",
    "    sigma = tf.tensordot(u, tf.linalg.matvec(tf.transpose(w), v), axes=1)\n",
    "    w_bar = w / sigma\n",
    "\n",
    "    return tf.reshape(w_bar, w_shape)\n",
    "\n",
    "# Create the Discriminator with Spectral Normalization\n",
    "def make_discriminator_model():\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    # Apply spectral normalization to the Dense layer\n",
    "    w = model.layers[-1].kernel\n",
    "    model.layers[-1].kernel = spectral_norm(w)\n",
    "    return model\n",
    "\n",
    "# Define loss and optimizers\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "generator = make_generator_model()\n",
    "discriminator = make_discriminator_model()\n",
    "\n",
    "# Define the training loop\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, 100])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for image_batch in dataset:\n",
    "            train_step(image_batch)\n",
    "        print(f\"Epoch {epoch + 1} completed\")\n",
    "\n",
    "# Train\n",
    "train(train_dataset, 10)\n",
    "\n",
    "# Display generated images\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "noise = tf.random.normal([9, 100])\n",
    "generated_images = generator(noise, training=False)\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(6,6))\n",
    "for i, img in enumerate(generated_images):\n",
    "    ax = axs[i // 3, i % 3]\n",
    "    ax.imshow(img.numpy().reshape(28,28) * 127.5 + 127.5, cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6c89ea-cee1-48dd-a414-d3b2b3beff72",
   "metadata": {},
   "source": [
    "### 486. Build a Multi-Objective Optimization Algorithm with Evolutionary Constrained Optimization and Custom Constraint Handling\n",
    "\n",
    "Multi-objective optimization is about finding solutions that balance multiple objectives. Evolutionary algorithms can be adapted to these scenarios. In this code, we will employ an evolutionary algorithm to handle multi-objective optimization with constraints.\n",
    "\n",
    "Problem Definition:\n",
    "\n",
    "1. Objectives: We'll consider a two-objective optimization problem for demonstration.\n",
    "- Objective 1: Minimize f1(x) = x2\n",
    "- Objective 2: Minimize f2(x) = (x-2)2\n",
    "2. Constraint: The solution must satisfy x>1.\n",
    "\n",
    "The ideal Pareto front for this problem lies between 1 and 2 on the x-axis.\n",
    "\n",
    "Expected Output:\n",
    "The output will be a list of x values which represent solutions that satisfy the given objectives and constraints. These solutions should ideally lie between 1 and 2 (inclusive of 1 but not of 2) since that's the Pareto front for this problem. The exact numbers will vary between runs because of the stochastic nature of the algorithm.\n",
    "\n",
    "Notes:\n",
    "\n",
    "This is a very basic version of an MOEA. Advanced techniques like NSGA-II, SPEA2, etc., are used in practice.\n",
    "The constraint handling done here is a basic penalty method, where infeasible solutions are penalized by increasing their objective values. In practice, sophisticated constraint-handling techniques might be applied.\n",
    "\n",
    "Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d4c256-6eb1-4067-8015-b0f3c63f820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define objectives\n",
    "def objective1(x):\n",
    "    return x**2\n",
    "\n",
    "def objective2(x):\n",
    "    return (x - 2)**2\n",
    "\n",
    "# Define constraints\n",
    "def constraint(x):\n",
    "    if x > 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Multi-objective Evolutionary Algorithm\n",
    "def moea(num_generations=1000, pop_size=100, mutation_rate=0.02):\n",
    "    # Initial population\n",
    "    population = np.random.rand(pop_size) * 10 - 5  # Random initialization between -5 and 5\n",
    "\n",
    "    for generation in range(num_generations):\n",
    "        # Evaluate objectives and constraints\n",
    "        obj1_values = np.array([objective1(x) for x in population])\n",
    "        obj2_values = np.array([objective2(x) for x in population])\n",
    "        constraint_values = np.array([constraint(x) for x in population])\n",
    "\n",
    "        # Select parents - preference given to feasible solutions (those satisfying constraints)\n",
    "        combined_scores = obj1_values + obj2_values - 5 * constraint_values  # Bias towards feasible solutions\n",
    "        selected_parents = population[np.argsort(combined_scores)[:pop_size // 2]]\n",
    "\n",
    "        # Crossover\n",
    "        children = []\n",
    "        while len(children) < pop_size // 2:\n",
    "            parent_choices = np.random.choice(selected_parents, 2, replace=False)\n",
    "            crossover_point = np.random.rand()\n",
    "            child = crossover_point * parent_choices[0] + (1 - crossover_point) * parent_choices[1]\n",
    "            children.append(child)\n",
    "        children = np.array(children)\n",
    "\n",
    "        # Mutation\n",
    "        mutation_mask = np.random.rand(pop_size // 2) < mutation_rate\n",
    "        children[mutation_mask] += np.random.randn(np.sum(mutation_mask)) * mutation_rate\n",
    "\n",
    "        # Combine parents and children and move to next generation\n",
    "        population = np.concatenate([selected_parents, children])\n",
    "\n",
    "    return population\n",
    "\n",
    "# Run MOEA\n",
    "population = moea()\n",
    "\n",
    "# Display final solutions\n",
    "print(\"Final solutions (x-values):\", population)\n",
    "\n",
    "# Note: You should find solutions close to the interval [1, 2] satisfying the constraints and objectives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f60e886-4fda-40a2-b40b-74f43d8bc244",
   "metadata": {},
   "source": [
    "### 487. Implement an Autoencoder with Denoising Autoencoder for Anomaly Detection and Custom Noise Level\n",
    "A denoising autoencoder is designed to learn how to encode the primary features of data in such a way that it can ignore noise. Here's an example using TensorFlow and Keras on the Digits dataset from sklearn. We'll consider the \"9\" digit as an anomaly and all others as normal data points.\n",
    "\n",
    "The example will:\n",
    "\n",
    "Load the dataset.\n",
    "Introduce noise to the data.\n",
    "Train a denoising autoencoder.\n",
    "Test for anomaly detection, with the assumption that anomalies will have higher reconstruction errors.\n",
    "\n",
    "pip install tensorflow scikit-learn matplotlib\n",
    "\n",
    "Expected Output:\n",
    "You will see two rows of images: the first row contains noisy versions of test images, and the second row contains the denoised versions produced by the autoencoder. The numbers are reconstructed to an extent where they are recognizable, which means the autoencoder is working effectively.\n",
    "\n",
    "The Number of '9' digits detected as anomalies output should tell you how many of the digit \"9\" were detected as anomalies.\n",
    "\n",
    "Notes:\n",
    "\n",
    "1. Here we used the mean squared error as a simple metric for anomaly detection. In practice, more sophisticated methods might be applied.\n",
    "2. Adjusting the noise level, adding more layers to the autoencoder, or changing the encoding dimensions can influence the model's ability to denoise the input and detect anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b3b6ac-9001-44d1-b9b6-70cd3d4c58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data / 16.0\n",
    "y = digits.target\n",
    "\n",
    "# Introduce noise\n",
    "def add_noise(data, noise_factor=0.5):\n",
    "    noisy_data = data + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=data.shape) \n",
    "    return np.clip(noisy_data, 0., 1.)\n",
    "\n",
    "X_noisy = add_noise(X, noise_factor=0.5)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, _, y_test = train_test_split(X_noisy, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the autoencoder\n",
    "input_layer = tf.keras.layers.Input(shape=(X.shape[1],))\n",
    "encoded = tf.keras.layers.Dense(32, activation='relu')(input_layer)\n",
    "decoded = tf.keras.layers.Dense(X.shape[1], activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = tf.keras.models.Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the autoencoder\n",
    "autoencoder.fit(X_train, X_train, epochs=100, batch_size=256, validation_data=(X_test, X_test), verbose=1)\n",
    "\n",
    "# Use autoencoder for anomaly detection\n",
    "X_test_denoised = autoencoder.predict(X_test)\n",
    "mse = np.mean(np.power(X_test - X_test_denoised, 2), axis=1)\n",
    "mse_threshold = np.quantile(mse, 0.999) # Get the 99.9% quantile as threshold\n",
    "\n",
    "# Check if number 9s have high reconstruction errors\n",
    "anomalies = y_test[mse > mse_threshold]\n",
    "anomaly_count = np.sum(anomalies == 9)\n",
    "\n",
    "print(f\"Number of '9' digits detected as anomalies: {anomaly_count} out of {np.sum(y_test == 9)}\")\n",
    "\n",
    "# Visualize\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Original noisy images\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(X_test[i].reshape(8, 8))\n",
    "    plt.gray()\n",
    "    ax.set_title(\"Original Noisy\")\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Reconstruction\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(X_test_denoised[i].reshape(8, 8))\n",
    "    plt.gray()\n",
    "    ax.set_title(\"Denoised\")\n",
    "    ax.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59a25f0-137b-49a7-ba00-4ec9b962bf7e",
   "metadata": {},
   "source": [
    "### 488. Create a Reinforcement Learning Agent using Conservative Policy Optimization with Custom Trust Region Size\n",
    "Conservative Policy Optimization (CPO) is an algorithm in the family of Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO), which is designed to handle constraints more robustly. For our example, we'll create a simple implementation of CPO on the CartPole environment from OpenAI's Gym.\n",
    "Setup:\n",
    "\n",
    "Firstly, ensure you have the necessary libraries:\n",
    "pip install gym tensorflow\n",
    "\n",
    "\n",
    "Expected Output:\n",
    "You should see an episode count with the corresponding reward. The reward should generally increase as episodes progress, indicating the agent is learning. For example:\n",
    "\n",
    "Episode: 1, Reward: 21.0\n",
    "Episode: 2, Reward: 19.0\n",
    "...\n",
    "Episode: 199, Reward: 200.0\n",
    "Episode: 200, Reward: 200.0\n",
    "This output suggests that the agent's performance has been improving over episodes, and by the end, it consistently achieves the maximum reward for the CartPole environment.\n",
    "\n",
    "Note:\n",
    "\n",
    "1. The trust_region_size hyperparameter controls the size of the region within which the updated policy is considered trustworthy. Adjusting this value can lead to more stable or faster learning.\n",
    "2. This is a simple implementation for illustrative purposes. Enhancements such as adding an actor-critic structure, adjusting learning rate, and using more advanced methods to handle the trust region can further improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730177cd-0cad-4aba-abdc-3539470284e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Environment Setup\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Neural Network Policy\n",
    "inputs = Input(shape=(state_dim,))\n",
    "fc1 = Dense(24, activation='relu')(inputs)\n",
    "fc2 = Dense(24, activation='relu')(fc1)\n",
    "probs = Dense(action_dim, activation='softmax')(fc2)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=probs)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "def get_action(state):\n",
    "    state = state.reshape([1, state_dim])\n",
    "    probs = model.predict(state)[0]\n",
    "    return np.random.choice(action_dim, p=probs)\n",
    "\n",
    "# Conservative Policy Optimization (CPO) Training\n",
    "def train(states, actions, rewards, trust_region_size=0.01):\n",
    "    discounted_rewards = []\n",
    "    cumulative_reward = 0\n",
    "    for reward in rewards[::-1]:\n",
    "        cumulative_reward = reward + cumulative_reward * 0.99\n",
    "        discounted_rewards.insert(0, cumulative_reward)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        for state, action, reward in zip(states, actions, discounted_rewards):\n",
    "            state = state.reshape([1, state_dim])\n",
    "            probs = model(state)\n",
    "            loss = -tf.math.log(probs[0, action]) * reward  # Negative log likelihood\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        \n",
    "        # Trust Region Modification\n",
    "        for i, gradient in enumerate(gradients):\n",
    "            gradients[i] = tf.clip_by_norm(gradient, trust_region_size)\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "# Main training loop\n",
    "for episode in range(200):\n",
    "    state = env.reset()\n",
    "    states, actions, rewards = [], [], []\n",
    "    episode_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        action = get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        if done:\n",
    "            train(states, actions, rewards)\n",
    "            print(f\"Episode: {episode + 1}, Reward: {episode_reward}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fc9cab-2b48-47ae-ba49-652f83006b33",
   "metadata": {},
   "source": [
    "### 489. Develop a Hybrid Recommender System with Collaborative Filtering and Sequential Recommendation\n",
    "The idea behind a hybrid recommender system that combines collaborative filtering and sequential recommendation is to leverage both general user-item interactions and specific sequential patterns to improve recommendation accuracy.\n",
    "\n",
    "In this example:\n",
    "\n",
    "We'll use collaborative filtering via matrix factorization (using a simple neural network) to learn latent user and item embeddings.\n",
    "We'll use a recurrent neural network (RNN) to capture sequential patterns of item interactions.\n",
    "We'll merge the two models to produce a final recommendation score.\n",
    "Let's use a simple dataset for this demonstration.\n",
    "\n",
    "Expected Output:\n",
    "Epoch 1/3\n",
    "...\n",
    "Epoch 2/3\n",
    "...\n",
    "Epoch 3/3\n",
    "...\n",
    "Note:\n",
    "1. This code sets up the hybrid model structure. The dataset is randomly generated for the sake of demonstration. In a real-world scenario, you would replace it with a genuine dataset.\n",
    "2. The collaborative filtering component is a basic matrix factorization technique implemented using embeddings.\n",
    "3. The sequential component uses an LSTM to capture the sequence of items. It's a very simple version, and there's room for enhancing it, such as considering timestamp, adding more LSTM layers, or using more sophisticated models like Transformers.\n",
    "4. The models are not fine-tuned. Hyperparameters like the number of embeddings, dense layer sizes, batch sizes, and learning rates should be adjusted based on the dataset and problem specifics.\n",
    "\n",
    "Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4116de-812c-46e0-8443-3240aee96113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Generate random data: 1000 users, 500 items, and 10000 interactions\n",
    "n_users = 1000\n",
    "n_items = 500\n",
    "n_interactions = 10000\n",
    "\n",
    "user_ids = np.random.randint(0, n_users, n_interactions)\n",
    "item_ids = np.random.randint(0, n_items, n_interactions)\n",
    "ratings = np.random.randint(1, 6, n_interactions)  # ratings between 1 to 5\n",
    "sequences = [np.random.choice(n_items, 5, replace=False) for _ in range(n_interactions)]  # sequences of 5 items\n",
    "\n",
    "# Collaborative Filtering Model\n",
    "user_input = layers.Input(shape=(1,))\n",
    "item_input = layers.Input(shape=(1,))\n",
    "\n",
    "user_embedding = layers.Embedding(n_users, 50)(user_input)\n",
    "item_embedding = layers.Embedding(n_items, 50)(item_input)\n",
    "\n",
    "user_flatten = layers.Flatten()(user_embedding)\n",
    "item_flatten = layers.Flatten()(item_embedding)\n",
    "\n",
    "merged_layer = layers.Concatenate()([user_flatten, item_flatten])\n",
    "dense_layer = layers.Dense(10, activation='relu')(merged_layer)\n",
    "output_cf = layers.Dense(1)(dense_layer)\n",
    "\n",
    "CF_Model = Model(inputs=[user_input, item_input], outputs=output_cf)\n",
    "\n",
    "# Sequential Model\n",
    "sequence_input = layers.Input(shape=(5,))\n",
    "sequence_embedding = layers.Embedding(n_items, 50)(sequence_input)\n",
    "sequence_lstm = layers.LSTM(50)(sequence_embedding)\n",
    "output_seq = layers.Dense(n_items, activation='softmax')(sequence_lstm)\n",
    "\n",
    "Seq_Model = Model(inputs=sequence_input, outputs=output_seq)\n",
    "\n",
    "# Hybrid Model\n",
    "combined_item_embedding = layers.Concatenate()([item_flatten, sequence_lstm])\n",
    "dense_combined = layers.Dense(10, activation='relu')(combined_item_embedding)\n",
    "output_hybrid = layers.Dense(1)(dense_combined)\n",
    "\n",
    "Hybrid_Model = Model(inputs=[user_input, item_input, sequence_input], outputs=output_hybrid)\n",
    "\n",
    "# Compile and train\n",
    "Hybrid_Model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "Hybrid_Model.fit([user_ids, item_ids, np.array(sequences)], ratings, epochs=3, batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c987c1e4-e286-4e2a-be21-e4e441509691",
   "metadata": {},
   "source": [
    "### 490. Build a Transfer Learning Model with Unsupervised Domain Adaptation and Custom Adversarial Loss\n",
    "Unsupervised Domain Adaptation (UDA) aims to leverage labeled data from a source domain to learn a model that performs well on an unlabeled target domain. A popular approach to UDA is to use adversarial training, where a domain discriminator tries to distinguish between source and target samples while the feature extractor tries to fool the discriminator.\n",
    "\n",
    "Here, we will build a Transfer Learning model with UDA using a custom adversarial loss.\n",
    "\n",
    "Outline:\n",
    "1. Setup: Import necessary libraries.\n",
    "2. Dataset: For simplicity, we'll use MNIST as the source domain and a modified version (e.g., color-inverted) as the target domain.\n",
    "3. Model Architecture: Create a feature extractor, a classifier, and a domain discriminator.\n",
    "4. Adversarial Loss: Custom loss to train the model adversarially.\n",
    "5. Training: Train using labeled source data and unlabeled target data.\n",
    "6. Evaluation: Test the domain-adapted model on target data.\n",
    "\n",
    "Expected Output:\n",
    "...\n",
    "Epoch 5/5\n",
    "...\n",
    "Target domain accuracy: ...\n",
    "Note:\n",
    "\n",
    "The above example uses a basic UDA setup with a simple adversarial loss for illustration purposes. In practice, more complex models and sophisticated losses can be employed.\n",
    "Using the full MNIST dataset for the source domain and inverted MNIST as the target domain is for demonstration purposes. In practice, you'd likely use two different datasets or more significant modifications to simulate domain shifts.\n",
    "This example assumes the source and target domains have the same number of classes and uses source labels as dummy labels for the target domain during adversarial training. In practice, careful handling might be required if the domains have different class distributions.\n",
    "\n",
    "\n",
    "Code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c068d2-6d37-44e7-b37b-3641a163461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# Load datasets\n",
    "(source_train_images, source_train_labels), (source_test_images, source_test_labels) = mnist.load_data()\n",
    "target_train_images = 255 - source_train_images  # Inverted MNIST as target\n",
    "target_test_images = 255 - source_test_images\n",
    "\n",
    "# Preprocessing\n",
    "source_train_images = source_train_images.astype('float32') / 255\n",
    "source_test_images = source_test_images.astype('float32') / 255\n",
    "target_train_images = target_train_images.astype('float32') / 255\n",
    "target_test_images = target_test_images.astype('float32') / 255\n",
    "\n",
    "source_train_images = np.expand_dims(source_train_images, axis=-1)\n",
    "source_test_images = np.expand_dims(source_test_images, axis=-1)\n",
    "target_train_images = np.expand_dims(target_train_images, axis=-1)\n",
    "target_test_images = np.expand_dims(target_test_images, axis=-1)\n",
    "\n",
    "# Define the feature extractor\n",
    "input_img = layers.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu')(input_img)\n",
    "x = layers.MaxPooling2D((2, 2))(x)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "features = layers.Flatten()(x)\n",
    "\n",
    "feature_extractor = Model(inputs=input_img, outputs=features)\n",
    "\n",
    "# Define the classifier\n",
    "classifier_input = layers.Input(shape=(features.shape[1],))\n",
    "x = layers.Dense(64, activation='relu')(classifier_input)\n",
    "classifier_output = layers.Dense(10, activation='softmax')(x)\n",
    "classifier = Model(inputs=classifier_input, outputs=classifier_output)\n",
    "\n",
    "# Define the domain discriminator\n",
    "discriminator_input = layers.Input(shape=(features.shape[1],))\n",
    "x = layers.Dense(64, activation='relu')(discriminator_input)\n",
    "discriminator_output = layers.Dense(1, activation='sigmoid')(x)  # Binary classification: source vs. target\n",
    "domain_discriminator = Model(inputs=discriminator_input, outputs=discriminator_output)\n",
    "\n",
    "# Combined model\n",
    "combined_features = feature_extractor(input_img)\n",
    "combined_class_predictions = classifier(combined_features)\n",
    "combined_domain_predictions = domain_discriminator(combined_features)\n",
    "combined_model = Model(inputs=input_img, outputs=[combined_class_predictions, combined_domain_predictions])\n",
    "\n",
    "# Custom adversarial loss\n",
    "def adversarial_loss(y_true, y_pred):\n",
    "    return tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "# Compile models\n",
    "classifier.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "domain_discriminator.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "combined_model.compile(optimizer='adam', loss=['sparse_categorical_crossentropy', adversarial_loss], loss_weights=[1, 0.5])\n",
    "\n",
    "# Train classifier on source data\n",
    "classifier.fit(feature_extractor.predict(source_train_images), source_train_labels, epochs=5)\n",
    "\n",
    "# Adversarial training\n",
    "for epoch in range(5):\n",
    "    # Domain labels: 0 for source, 1 for target\n",
    "    source_domain_labels = np.zeros(source_train_images.shape[0])\n",
    "    target_domain_labels = np.ones(target_train_images.shape[0])\n",
    "    \n",
    "    # Train on source\n",
    "    combined_model.train_on_batch(source_train_images, [source_train_labels, source_domain_labels])\n",
    "    \n",
    "    # Train on target\n",
    "    combined_model.train_on_batch(target_train_images, [source_train_labels, target_domain_labels])  # Using source labels as dummy labels for classification loss\n",
    "\n",
    "# Evaluate on target domain\n",
    "target_features = feature_extractor.predict(target_test_images)\n",
    "accuracy = classifier.evaluate(target_features, source_test_labels)[1]\n",
    "print(f\"Target domain accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5435a2-d0fd-4882-a5cd-739d4435ab7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
